"""Minimal transformer."""
import jax
import jax.numpy as jnp
from jax import random
from jax.nn import initializers
from jax.scipy.special import logsumexp # A numerically stable way to compute log(sum(exp(x)))

# A stable GELU activation function implementation
# Gaussian Error Linear Unit is a smooth activation often used in transformers.
def gelu(x):
    """Gaussian Error Linear Unit activation function."""
    return 0.5 * x * (1 + jnp.tanh(jnp.sqrt(2 / jnp.pi) * (x + 0.044715 * x**3)))

#---------------------------------------------------#
## 1. Positional Encoding
#---------------------------------------------------#
# Since the model contains no recurrence or convolution, we must inject information
# about the relative or absolute position of the tokens in the sequence.
def positional_encoding(seq_len, d_model):
    """
    Generates sinusoidal positional encodings.

    Args:
        seq_len (int): The length of the sequence.
        d_model (int): The dimensionality of the model's embeddings.

    Returns:
        jnp.ndarray: A (1, seq_len, d_model) array of positional encodings.
    """
    # Create an array of position indices [0, 1, ..., seq_len-1]
    position = jnp.arange(seq_len)[:, jnp.newaxis]
    # Create an array for the division term in the formula
    div_term = jnp.exp(jnp.arange(0, d_model, 2) * -(jnp.log(10000.0) / d_model))

    # Calculate the positional encodings
    pe = jnp.zeros((seq_len, d_model))
    # Apply sin to even indices in the array; 2i
    pe = pe.at[:, 0::2].set(jnp.sin(position * div_term))
    # Apply cos to odd indices in the array; 2i+1
    pe = pe.at[:, 1::2].set(jnp.cos(position * div_term))

    # Add a batch dimension so it can be easily added to the input embeddings
    return pe[jnp.newaxis, :, :]

#---------------------------------------------------#
## 2. Scaled Dot-Product Attention
#---------------------------------------------------#
# This is the core attention mechanism. It computes a weighted sum of the values,
# where the weight of each value is determined by the dot-product of the query
# with the corresponding key.
def scaled_dot_product_attention(q, k, v, mask=None):
    """
    Computes Scaled Dot-Product Attention.

    Args:
        q (jnp.ndarray): Queries, shape (..., seq_len_q, d_k)
        k (jnp.ndarray): Keys, shape (..., seq_len_k, d_k)
        v (jnp.ndarray): Values, shape (..., seq_len_k, d_v)
        mask (jnp.ndarray, optional): Mask to apply to the attention scores.

    Returns:
        tuple: A tuple containing the output of the attention and the attention weights.
    """
    # Matmul: (..., seq_len_q, d_k) @ (..., d_k, seq_len_k) -> (..., seq_len_q, seq_len_k)
    matmul_qk = jnp.matmul(q, jnp.swapaxes(k, -2, -1))

    # Scale matmul_qk by the square root of the key dimension (d_k)
    d_k = q.shape[-1]
    scaled_attention_logits = matmul_qk / jnp.sqrt(d_k)

    # Apply the mask (if provided). The mask is used to prevent attention
    # to certain positions (e.g., padding tokens or future tokens in a decoder).
    if mask is not None:
        # We add a very large negative number to the masked positions.
        # When softmax is applied, these positions will have a weight of ~0.
        scaled_attention_logits += (mask * -1e9)

    # Softmax is applied to the last axis (seq_len_k) to obtain the weights.
    attention_weights = jax.nn.softmax(scaled_attention_logits, axis=-1)

    # The weights are multiplied by the values to get the final output.
    # Output shape: (..., seq_len_q, d_v)
    output = jnp.matmul(attention_weights, v)
    return output, attention_weights

#---------------------------------------------------#
## 3. Multi-Head Attention
#---------------------------------------------------#
# Multi-head attention allows the model to jointly attend to information from
# different representation subspaces at different positions.
def multi_head_attention_init_params(key, d_model, num_heads):
    """Initializes parameters for the Multi-Head Attention layer."""
    assert d_model % num_heads == 0, "d_model must be divisible by num_heads"
    d_k = d_model // num_heads
    keys = random.split(key, 4)

    # Initialize weights for Q, K, V, and the final output projection
    # glorot_uniform is a standard way to initialize neural network weights.
    wq = initializers.glorot_uniform()(keys[0], (d_model, d_model))
    wk = initializers.glorot_uniform()(keys[1], (d_model, d_model))
    wv = initializers.glorot_uniform()(keys[2], (d_model, d_model))
    wo = initializers.glorot_uniform()(keys[3], (d_model, d_model))

    return {"wq": wq, "wk": wk, "wv": wv, "wo": wo}

def multi_head_attention_apply(params, x_q, x_k, x_v, mask, d_model, num_heads):
    """Applies the Multi-Head Attention layer."""
    batch_size = x_q.shape[0]
    d_k = d_model // num_heads

    # 1. Linearly project Q, K, V
    # x shape: (batch_size, seq_len, d_model)
    # W shape: (d_model, d_model)
    # Q, K, V shape: (batch_size, seq_len, d_model)
    q = jnp.matmul(x_q, params["wq"])
    k = jnp.matmul(x_k, params["wk"])
    v = jnp.matmul(x_v, params["wv"])

    # 2. Reshape and transpose for multi-head computation
    # This splits the d_model dimension into (num_heads, d_k)
    # New shape: (batch_size, num_heads, seq_len, d_k)
    q = q.reshape(batch_size, -1, num_heads, d_k).transpose(0, 2, 1, 3)
    k = k.reshape(batch_size, -1, num_heads, d_k).transpose(0, 2, 1, 3)
    v = v.reshape(batch_size, -1, num_heads, d_k).transpose(0, 2, 1, 3)

    # 3. Apply scaled dot-product attention
    # attention shape: (batch_size, num_heads, seq_len_q, d_k)
    attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)

    # 4. Concatenate heads and apply final linear layer
    # Transpose back to (batch_size, seq_len_q, num_heads, d_k)
    attention = attention.transpose(0, 2, 1, 3)
    # Reshape to (batch_size, seq_len_q, d_model) to concatenate heads
    concat_attention = attention.reshape(batch_size, -1, d_model)

    # Final linear projection
    # output shape: (batch_size, seq_len_q, d_model)
    output = jnp.matmul(concat_attention, params["wo"])
    return output, attention_weights

#---------------------------------------------------#
## 4. Position-wise Feed-Forward Network
#---------------------------------------------------#
# This is applied to each position independently. It consists of two linear
# transformations with a GELU activation in between.
def feed_forward_init_params(key, d_model, d_ff):
    """Initializes parameters for the Feed-Forward Network."""
    keys = random.split(key, 2)
    # Weights for the two linear layers
    w1 = initializers.glorot_uniform()(keys[0], (d_model, d_ff))
    w2 = initializers.glorot_uniform()(keys[1], (d_ff, d_model))
    # Biases for the two linear layers
    b1 = jnp.zeros(d_ff)
    b2 = jnp.zeros(d_model)
    return {"w1": w1, "b1": b1, "w2": w2, "b2": b2}

def feed_forward_apply(params, x):
    """Applies the Feed-Forward Network."""
    # First linear transformation with GELU activation
    x = gelu(jnp.matmul(x, params["w1"]) + params["b1"])
    # Second linear transformation
    x = jnp.matmul(x, params["w2"]) + params["b2"]
    return x

#---------------------------------------------------#
## 5. Layer Normalization
#---------------------------------------------------#
# Used to stabilize training by normalizing the inputs to each sub-layer.
def layer_norm_init_params(d_model, eps=1e-5):
    """Initializes parameters for Layer Normalization."""
    # These are learnable parameters, sometimes called gain (gamma) and bias (beta)
    gamma = jnp.ones(d_model)
    beta = jnp.zeros(d_model)
    return {"gamma": gamma, "beta": beta, "eps": eps}

def layer_norm_apply(params, x):
    """Applies Layer Normalization."""
    mean = jnp.mean(x, axis=-1, keepdims=True)
    std = jnp.std(x, axis=-1, keepdims=True)
    return params["gamma"] * (x - mean) / (std + params["eps"]) + params["beta"]

#---------------------------------------------------#
## 6. Encoder Layer
#---------------------------------------------------#
# An encoder layer consists of a multi-head self-attention mechanism followed by
# a position-wise feed-forward network. Residual connections and layer normalization
# are applied around each of the two sub-layers.
def encoder_layer_init_params(key, d_model, num_heads, d_ff):
    """Initializes parameters for a single Encoder Layer."""
    mha_key, ffn_key = random.split(key)
    return {
        "mha": multi_head_attention_init_params(mha_key, d_model, num_heads),
        "ffn": feed_forward_init_params(ffn_key, d_model, d_ff),
        "norm1": layer_norm_init_params(d_model),
        "norm2": layer_norm_init_params(d_model)
    }

def encoder_layer_apply(params, x, mask, d_model, num_heads):
    """Applies a single Encoder Layer."""
    # 1. Multi-Head Attention sub-layer
    attn_output, _ = multi_head_attention_apply(
        params["mha"], x, x, x, mask, d_model, num_heads
    )
    # Residual connection and layer normalization
    x = layer_norm_apply(params["norm1"], x + attn_output)

    # 2. Feed-Forward sub-layer
    ffn_output = feed_forward_apply(params["ffn"], x)
    # Residual connection and layer normalization
    x = layer_norm_apply(params["norm2"], x + ffn_output)
    return x

#---------------------------------------------------#
## 7. Decoder Layer
#---------------------------------------------------#
# A decoder layer has three sub-layers: masked self-attention, encoder-decoder
# attention, and a feed-forward network.
def decoder_layer_init_params(key, d_model, num_heads, d_ff):
    """Initializes parameters for a single Decoder Layer."""
    mha1_key, mha2_key, ffn_key = random.split(key, 3)
    return {
        "mha1": multi_head_attention_init_params(mha1_key, d_model, num_heads),
        "mha2": multi_head_attention_init_params(mha2_key, d_model, num_heads),
        "ffn": feed_forward_init_params(ffn_key, d_model, d_ff),
        "norm1": layer_norm_init_params(d_model),
        "norm2": layer_norm_init_params(d_model),
        "norm3": layer_norm_init_params(d_model)
    }

def decoder_layer_apply(params, x, enc_output, look_ahead_mask, padding_mask, d_model, num_heads):
    """Applies a single Decoder Layer."""
    # 1. Masked Multi-Head Attention (self-attention)
    attn1, _ = multi_head_attention_apply(
        params["mha1"], x, x, x, look_ahead_mask, d_model, num_heads
    )
    x = layer_norm_apply(params["norm1"], x + attn1)

    # 2. Encoder-Decoder Attention
    # Query is from the previous decoder layer (x), Key and Value are from the encoder output.
    attn2, _ = multi_head_attention_apply(
        params["mha2"], x, enc_output, enc_output, padding_mask, d_model, num_heads
    )
    x = layer_norm_apply(params["norm2"], x + attn2)

    # 3. Feed-Forward Network
    ffn_output = feed_forward_apply(params["ffn"], x)
    x = layer_norm_apply(params["norm3"], x + ffn_output)
    return x

#---------------------------------------------------#
## 8. Full Transformer Model
#---------------------------------------------------#
# The Transformer model stacks N encoder layers and N decoder layers.
def transformer_init_params(key, num_layers, vocab_size, d_model, num_heads, d_ff):
    """Initializes parameters for the entire Transformer model."""
    keys = random.split(key, num_layers * 2 + 2)
    return {
        "encoder_embedding": initializers.glorot_uniform()(keys[0], (vocab_size, d_model)),
        "decoder_embedding": initializers.glorot_uniform()(keys[1], (vocab_size, d_model)),
        "encoder_layers": [encoder_layer_init_params(keys[i+2], d_model, num_heads, d_ff) for i in range(num_layers)],
        "decoder_layers": [decoder_layer_init_params(keys[i+2+num_layers], d_model, num_heads, d_ff) for i in range(num_layers)],
        "final_linear": initializers.glorot_uniform()(keys[-1], (d_model, vocab_size))
    }

def transformer_apply(params, src, tgt, src_mask, tgt_mask, num_layers, d_model, num_heads):
    """Applies the full Transformer model (forward pass)."""
    # 1. Get sequence lengths and apply embeddings
    src_seq_len = src.shape[1]
    tgt_seq_len = tgt.shape[1]
    
    # Encoder input embedding and positional encoding
    enc_output = params["encoder_embedding"][src, :] * jnp.sqrt(d_model)
    enc_output += positional_encoding(src_seq_len, d_model)
    
    # Decoder input embedding and positional encoding
    dec_output = params["decoder_embedding"][tgt, :] * jnp.sqrt(d_model)
    dec_output += positional_encoding(tgt_seq_len, d_model)

    # 2. Encoder Stack
    for i in range(num_layers):
        enc_output = encoder_layer_apply(params["encoder_layers"][i], enc_output, src_mask, d_model, num_heads)

    # 3. Decoder Stack
    for i in range(num_layers):
        dec_output = decoder_layer_apply(params["decoder_layers"][i], dec_output, enc_output, tgt_mask, src_mask, d_model, num_heads)
        
    # 4. Final Linear layer and Softmax
    final_output = jnp.matmul(dec_output, params["final_linear"])
    
    # We return the logits. Softmax would be applied in the loss function for training.
    return final_output

#---------------------------------------------------#
## 9. Utility: Mask Creation
#---------------------------------------------------#
def create_look_ahead_mask(size):
    """
    Creates a look-ahead mask for the decoder's self-attention.
    This prevents positions from attending to subsequent positions.
    Returns a mask of shape (1, 1, size, size)
    """
    mask = 1 - jnp.tril(jnp.ones((size, size)))
    return mask[jnp.newaxis, jnp.newaxis, :, :] # Add batch and head dimensions

#---------------------------------------------------#
## 10. Example Usage
#---------------------------------------------------#
if __name__ == '__main__':
    # Hyperparameters
    key = random.PRNGKey(0)
    batch_size = 2
    src_vocab_size = 1000
    tgt_vocab_size = 1200
    src_seq_len = 10
    tgt_seq_len = 12
    num_layers = 4      # Number of encoder/decoder layers
    d_model = 128       # Embedding dimension
    num_heads = 8       # Number of attention heads
    d_ff = 512          # Hidden layer size in FFN

    # 1. Initialize model parameters
    print("Initializing model parameters...")
    model_params = transformer_init_params(
        key, num_layers, src_vocab_size, d_model, num_heads, d_ff
    )
    print("Done.\n")
    
    # 2. Generate dummy input data
    key, src_key, tgt_key = random.split(key, 3)
    dummy_src = random.randint(src_key, (batch_size, src_seq_len), 0, src_vocab_size)
    dummy_tgt = random.randint(tgt_key, (batch_size, tgt_seq_len), 0, tgt_vocab_size)

    print(f"Source input shape: {dummy_src.shape}")
    print(f"Target input shape: {dummy_tgt.shape}\n")

    # 3. Create masks
    # For this minimal example, we'll use a simple look-ahead mask for the target
    # and no padding mask for the source. In a real application, you would also
    # create a padding mask for the source input.
    src_padding_mask = None # Not implemented for simplicity
    look_ahead_mask = create_look_ahead_mask(tgt_seq_len)
    print(f"Look-ahead mask shape: {look_ahead_mask.shape} (for target self-attention)\n")
    
    # 4. JIT compile the apply function for performance
    # JAX's Just-In-Time (JIT) compilation will dramatically speed up execution.
    # The first run will be slow due to compilation, but subsequent runs will be fast.
    print("JIT compiling the model's forward pass...")
    fast_transformer_apply = jax.jit(transformer_apply, static_argnums=(4, 5, 6))
    print("Done.\n")

    # 5. Run the forward pass
    print("Running a forward pass...")
    output_logits = fast_transformer_apply(
        model_params, dummy_src, dummy_tgt, src_padding_mask, look_ahead_mask,
        num_layers, d_model, num_heads
    )
    print("Done.\n")
    
    # 6. Check the output
    # The output should have the shape (batch_size, target_seq_len, target_vocab_size)
    print(f"Output logits shape: {output_logits.shape}")
    print(f"Expected shape: ({batch_size}, {tgt_seq_len}, {tgt_vocab_size})")

    # Verify that the output shape is as expected
    assert output_logits.shape == (batch_size, tgt_seq_len, tgt_vocab_size)

    print("\n✅ Minimal Transformer implementation in JAX ran successfully!")
